(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

;; Use URL local por padr√£o, mas permita substitui√ß√£o por URL do container
(def ^:private ollama-base-url (atom "http://localhost:11434"))
(def ^:private model-name "deepseek-r1")
(def ^:private max-context-length 4000)

(defn set-ollama-docker-mode!
  "Configura o uso do Ollama em modo Docker/Podman"
  []
  (reset! ollama-base-url "http://ollama:11434")
  (println "‚úÖ Configurado para usar Ollama dentro do cont√™iner Docker/Podman"))

(defn- api-error-message
  "Formata mensagem de erro da API"
  [response]
  (str "Erro ao chamar a API do Ollama: " 
       (:status response) " - " 
       (:body response)))

(defn- truncate-context
  "Limita o contexto a um tamanho m√°ximo para evitar problemas com o modelo"
  [context max-length]
  (if (<= (count context) max-length)
    context
    (let [half-length (int (/ max-length 2))
          start (subs context 0 half-length)
          end (subs context (- (count context) half-length))]
      (str start "\n...\n[Conte√∫do truncado para melhor processamento]\n...\n" end))))

(defn- try-single-url
  "Tenta fazer uma √∫nica chamada para uma URL espec√≠fica"
  [url options]
  (try
    (let [response @(http/post url options)]
      (if (= (:status response) 200)
        {:success true
         :result (-> response
                     :body
                     (json/read-str :key-fn keyword)
                     :response)}
        {:success false
         :error (api-error-message response)}))
    (catch Exception e
      {:success false
       :error (str "Falha ao conectar em " url ": " (.getMessage e))})))

(defn call-ollama-api
  "Chama a API do Ollama para gerar uma resposta baseada no prompt fornecido"
  [prompt]
  (let [primary-url (str @ollama-base-url "/api/generate")
        _ (println "DEBUG - Usando URL do Ollama:" primary-url)
        request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {"Content-Type" "application/json"}
                 :body (json/write-str request-body)
                 :timeout 120000}  ;; Aumentar o timeout para 2 minutos
        
        ;; Tentar primeiro com a URL prim√°ria
        primary-result (try-single-url primary-url options)]
    
    (if (:success primary-result)
      (:result primary-result)
      (do
        (println "‚ö†Ô∏è Erro na chamada prim√°ria:" (:error primary-result))
        (println "üîÑ Tentando URLs alternativas...")
        
        ;; Lista ampliada de poss√≠veis URLs para Ollama
        (let [alternative-hosts ["http://pgai-ollama-1:11434"   ;; Nome do cont√™iner no compose
                                 "http://ollama:11434"          ;; Nome curto do servi√ßo
                                 "http://172.18.0.2:11434"      ;; Poss√≠vel IP interno
                                 "http://host.docker.internal:11434"  ;; Mapeamento especial para Docker Desktop
                                 "http://localhost:11434"]      ;; Localhost
              results (atom [])]
          
          ;; Tenta cada URL at√© uma ter sucesso
          (doseq [host alternative-hosts]
            (when (empty? @results)
              (let [alt-url (str host "/api/generate")
                    _ (println "üîÑ Tentando conectar ao Ollama em" alt-url)
                    result (try-single-url alt-url options)]
                (if (:success result)
                  (do
                    (println "‚úÖ Conex√£o bem-sucedida com" alt-url)
                    (swap! results conj (:result result)))
                  (println "‚ö†Ô∏è Erro ao chamar a API do Ollama:" (:error result))))))
          
          (if (seq @results)
            (first @results)
            (do
              (println "‚ùå Todas as tentativas de conex√£o com Ollama falharam.")
              (str "N√£o foi poss√≠vel conectar ao Ollama usando nenhum dos endpoints dispon√≠veis. "
                   "Verifique se o servi√ßo Ollama est√° em execu√ß√£o e acess√≠vel. "
                   "\n\nErro encontrado: O modelo foi encontrado mas n√£o foi poss√≠vel conectar ao servi√ßo Ollama "
                   "para processar sua consulta."))))))))

;; Fun√ß√µes de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de c√≥digo do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks "```clojure\n(+ 1 2)\n```") => ["(+ 1 2)"]
;;
;; extract-summary: Cria um resumo de texto com tamanho m√°ximo especificado
;; exemplo de uso:
;;   (extract-summary "# T√≠tulo\nConte√∫do longo..." 50) => "Conte√∫do longo..."

(defn format-prompt
  "Formata o prompt para o LLM com o contexto e a consulta do usu√°rio"
  [context query]
  (let [truncated-context (truncate-context context max-context-length)]
    (str "Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. Com base no seguinte contexto da documenta√ß√£o:\n\n"
         truncated-context
         "\n\nPergunta: " query
         "\n\nForne√ßa uma resposta t√©cnica precisa e, se poss√≠vel, inclua exemplos de c√≥digo. "
         "Se a documenta√ß√£o n√£o contiver informa√ß√µes relevantes para a pergunta, "
         "indique isso claramente e forne√ßa uma resposta geral com base em seu conhecimento.")))

(defn generate-response
  "Gera resposta usando o LLM com base no contexto e na consulta do usu√°rio"
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println "DEBUG - Enviando prompt para o Ollama usando o modelo" model-name)
      (println "DEBUG - Tamanho do prompt ap√≥s truncamento:" (count prompt) "caracteres")
      (call-ollama-api prompt))
    (catch Exception e
      (str "Erro ao gerar resposta: " (.getMessage e) 
           "\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo " 
           @ollama-base-url
           "\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve")))) 